{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz1N-R2_lZXU"
   },
   "source": [
    "## **Working with Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1QOfSodb3Nx"
   },
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "- Import the required libraries\n",
    "- Load the Boston housing data set\n",
    "- Prepare the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://towardsdatascience.com/things-you-didnt-know-about-the-boston-housing-dataset-2e87a6f960e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f2bWUm6Rb3N0"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_openml\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# https://www.openml.org/search?type=data&sort=runs&status=active\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "# https://www.openml.org/search?type=data&sort=runs&status=active\n",
    "\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGzEXIplb3N1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the Boston dataset\n",
    "## boston = fetch_openml(name='boston', version=1, as_frame=True, parser='auto')\n",
    "#boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "feature_names = boston.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibc51nHjd2CV"
   },
   "source": [
    "## Step 2: Create a DataFrame and Check for Missing Values\n",
    "\n",
    "- Create a DataFrame using the Boston housing data\n",
    "- Display basic statistics\n",
    "- Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVV5gD0Cb3N1",
    "outputId": "a34b2108-60c7-435a-b484-dc3cc9a5554a"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['HousePrice'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lRXzMVsRl3l"
   },
   "source": [
    "__Observation__:\n",
    "\n",
    "- This is the head of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwGH3zYVb3N2",
    "outputId": "86e47772-1a8f-4e9d-c8f3-e78cba8b405c"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Myu1DvhsR-tI"
   },
   "source": [
    "__Observation__:\n",
    "- Here, you can see the statistical analysis of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6HKwO5ub3N3",
    "outputId": "afb54069-c3fb-42d5-c6f8-ae27866af72c"
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oW0CDYQSRAF"
   },
   "source": [
    "__Observation__:\n",
    "\n",
    "- There are no empty rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3K1ODY7d-1B"
   },
   "source": [
    "## Step 3: Remove Outliers from the HousePrice Column\n",
    "\n",
    "\n",
    "- Use a boxplot to visualize the outliers\n",
    "- Remove outliers from the __HousePrice__ column using the 1% and 99% quantiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k9RGwnZb3N3",
    "outputId": "22fc3226-cc45-40c8-ab5c-38eda94e2a0c"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df['HousePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZp3K8QKSdZK"
   },
   "source": [
    "__Observation__:\n",
    "\n",
    "- There are outliers in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN1zIWAxb3N3"
   },
   "outputs": [],
   "source": [
    "upper_limit = df['HousePrice'].quantile(0.99)\n",
    "lower_limit = df['HousePrice'].quantile(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZRLEq0Ob3N4"
   },
   "outputs": [],
   "source": [
    "df['HousePrice'] = np.where(df['HousePrice'] < lower_limit, lower_limit, df['HousePrice'])\n",
    "df['HousePrice'] = np.where(df['HousePrice'] > upper_limit, upper_limit, df['HousePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mEcChrUb3N4"
   },
   "source": [
    "## Step 4: Test for Linearity of the Model\n",
    "\n",
    "- Fit the model and display the summary\n",
    "- Define functions to calculate residuals and plot actual vs. predicted values\n",
    "- Test for linearity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAqXRl2Tb3N4",
    "outputId": "109dec1e-f200-423b-d549-ef7a0a6a5b83"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "boston.data = boston.data.apply(pd.to_numeric)\n",
    "\n",
    "# X_constant = sm.add_constant(np.asarray(boston.data))\n",
    "X_constant = sm.add_constant(boston.data)\n",
    "\n",
    "# X_constant is the constant of the model (b)\n",
    "# y = mX+b\n",
    "# It will add 1 to each value in the dataset.\n",
    "\n",
    "# boston_model = sm.OLS(boston.target, np.asarray(boston.data)).fit()\n",
    "boston_model = sm.OLS(boston.target, X_constant).fit()\n",
    "boston_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The independent variables:\n",
    "# CRIM          0\n",
    "# ZN            0\n",
    "# INDUS         0\n",
    "# CHAS          0\n",
    "# NOX           0\n",
    "# RM            0\n",
    "# AGE           0\n",
    "# DIS           0\n",
    "# RAD           0\n",
    "# TAX           0\n",
    "# PTRATIO       0\n",
    "# B             0\n",
    "# LSTAT         0\n",
    "\n",
    "## The dependent variable:\n",
    "# HousePrice    0\n",
    "\n",
    "## The model\n",
    "# y = coef+ax1+bx2+cx3+dx4+ex5+...nx13\n",
    "\n",
    "# the weights of the model represent the importance, influence, or correlation of the independent variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aguo5LjvoeXp"
   },
   "source": [
    "- Define a function to calculate residual values by taking the actual and predicted values\n",
    "- The value of residuals is equal to the difference between the actual and  predicted values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQUh_OUPb3N5"
   },
   "outputs": [],
   "source": [
    "def calculate_residuals(model, features, label):\n",
    "    predictions =  model.predict(features)\n",
    "    df_results = pd.DataFrame({'Actual' : label, 'Predicted' : predictions})\n",
    "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ1Cwq1oo3bH"
   },
   "source": [
    "- Next, define a function to plot the actual and predicted values using __lmplot__.\n",
    "- The orange line will show the fitted line created by the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MK9yWdfb3N5"
   },
   "outputs": [],
   "source": [
    "def linear_assumptions(model, features, label):\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "\n",
    "    sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, height=7)\n",
    "    line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
    "    plt.plot(line_coords, line_coords, color='darkorange', linestyle='--')\n",
    "    plt.title('Actual vs. Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81fWml2JpgsP"
   },
   "source": [
    "- Now, run the function __linear_assumptions__ to show the graph with the model as __boston_model__, features as __boston.data__, and label as __boston.taget__ variables as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_constant = sm.add_constant(np.asarray(boston.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSJXWOmqb3N5",
    "outputId": "72063509-c60c-4193-922b-9e41ef31e7e2"
   },
   "outputs": [],
   "source": [
    "# linear_assumptions(boston_model, boston.data, boston.target)\n",
    "linear_assumptions(boston_model, X_constant, boston.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1QeMWd_Sx8D"
   },
   "source": [
    "__Observation__:\n",
    "\n",
    "- We can observe that the line does not represent all the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ64i1QvebOr"
   },
   "source": [
    "## Step 5: Check for Multicollinearity\n",
    "\n",
    "- Let's check the correlation between the variables in the data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_Hez1HTb3N6",
    "outputId": "8f819fda-b933-4d20-8932-df6ecd9d93e0"
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVjEhxwjp9fQ"
   },
   "source": [
    "- Calculate the variance inflation factor (VIF) for each feature\n",
    "- Import the __variance_inflation_factor__ module from the __statsmodels.stats.outliers_influence__ library\n",
    "- Set the features as the DataFrame, except the target variable\n",
    "- Assign the __vif_data__ to the feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7CHlcPYb3N6",
    "outputId": "643807fe-e355-4015-c028-84cfc515d0fb"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x = df.drop(['HousePrice'], axis=1)\n",
    "x = x.astype(float)  # Convert the array to float type\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Features'] = x.columns\n",
    "\n",
    "vif_data['vif'] = [variance_inflation_factor(x.values, i) for i in range(len(x.columns))]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL40hTUwTCBW"
   },
   "source": [
    "__Observation__:\n",
    "- From the above output, we can infer that the columns **NOX, RM, AGE,** and **PTRATIO** have higher multicollinearity. Hence, we can drop them.Â "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1vuRpFCefRs"
   },
   "source": [
    "## Step 6: Remove Multicollinear Features and Split the Data set\n",
    "\n",
    "- Remove highly multicollinear features from the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvyyBRtQb3N6"
   },
   "outputs": [],
   "source": [
    "df1 = df.drop(['NOX', 'RM', 'AGE', 'PTRATIO'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph0xpq82qc-Q"
   },
   "source": [
    "- Now, set the feature and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehssSh0qb3N6"
   },
   "outputs": [],
   "source": [
    "x = df1.drop(['HousePrice'], axis =1) #features\n",
    "y = df1['HousePrice'] #target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT81ot6jqiew"
   },
   "source": [
    "- Next, split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3U5AxGSb3N7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the data\n",
    "X_train, X_test, y_train, y_test  = train_test_split(x, y, random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UBAT-l1e19z"
   },
   "source": [
    "\n",
    "## Step 7: Fit the Model and Evaluate Performance\n",
    "\n",
    "- Fit the model using OLS and display the summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz7MY5keb3N7",
    "outputId": "6eda9551-7764-448d-aeff-bad4ef5b0d17"
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print(X_train.dtype)\n",
    "print(y_train.dtype)\n",
    "print(np.isnan(X_train).sum())\n",
    "print(np.isnan(y_train).sum())\n",
    "\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not included a coefficient terms so the R2 might not be as accurate as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_oMwiNrq6wj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRE4WtfLrR1I"
   },
   "source": [
    "- Now, fit the model using linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LP9yZhRkb3N7"
   },
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = reg.predict(X_train)\n",
    "y_pred_test = reg.predict(X_test)\n",
    "\n",
    "X_test = pd.DataFrame(X_test)  # Convert X_test to a pandas DataFrame\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric values to NaN\n",
    "X_test = np.asarray(X_test)  # Convert X_test to a numpy array\n",
    "\n",
    "y_pred_test = reg.predict(X_test)  # Predict using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6-F069arUDW"
   },
   "source": [
    "- Evaluate the model using various metrics such as the **r2_score, mean_absolute_error, and mean_squared_error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for Train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k4xmEqeb3N8",
    "outputId": "65b17121-b20e-46ff-a536-25bdce70dc08"
   },
   "outputs": [],
   "source": [
    "print(\"R Square: {}\".format(r2_score(y_train, y_pred_train)))\n",
    "#the higher the value the better (close to 1)\n",
    "\n",
    "print(\"MAE: {}\".format(mean_absolute_error(y_train, y_pred_train)))\n",
    "print(\"MSE: {}\".format(mean_squared_error(y_train, y_pred_train)))\n",
    "# the lower the values the better.\n",
    "# this are not so good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhZgiVzgTc9b"
   },
   "source": [
    "__Observation__:\n",
    "\n",
    "- From the above output, we can observe that the model is a moderate fit for the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiYT-wChb3N8",
    "outputId": "8509410f-2c52-41ef-ca2a-afbb17152c60"
   },
   "outputs": [],
   "source": [
    "print(\"R Square: {}\".format(r2_score(y_test, y_pred_test)))\n",
    "print(\"MAE: {}\".format(mean_absolute_error(y_test, y_pred_test)))\n",
    "print(\"MSE: {}\".format(mean_squared_error(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AxD0wfGUFPi"
   },
   "source": [
    "__Observation__:\n",
    "\n",
    "- The model moderately explains the testing data, as indicated by the **R** **Square** value. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
